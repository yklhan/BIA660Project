{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import lda\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_file(file_name):\n",
    "    with open(file_name, 'r') as reader:\n",
    "        line_list = reader.readlines()\n",
    "    line_list = [x.strip() for x in line_list]\n",
    "    return line_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Big company with a small company feel. Very entrepreneurial. Management facilitates your ideas that push the business forward\n"
     ]
    }
   ],
   "source": [
    "pros =read_from_file('project_pro.txt')\n",
    "cons = read_from_file('project_con.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 100\n",
      "INFO:lda:vocab_size: 153\n",
      "INFO:lda:n_words: 679\n",
      "INFO:lda:n_topics: 5\n",
      "INFO:lda:n_iter: 500\n",
      "D:\\miniconda\\envs\\tf\\lib\\site-packages\\lda\\utils.py:55: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if sparse and not np.issubdtype(doc_word.dtype, int):\n",
      "INFO:lda:<0> log likelihood: -5953\n",
      "INFO:lda:<10> log likelihood: -3976\n",
      "INFO:lda:<20> log likelihood: -3932\n",
      "INFO:lda:<30> log likelihood: -3882\n",
      "INFO:lda:<40> log likelihood: -3878\n",
      "INFO:lda:<50> log likelihood: -3843\n",
      "INFO:lda:<60> log likelihood: -3833\n",
      "INFO:lda:<70> log likelihood: -3853\n",
      "INFO:lda:<80> log likelihood: -3867\n",
      "INFO:lda:<90> log likelihood: -3842\n",
      "INFO:lda:<100> log likelihood: -3881\n",
      "INFO:lda:<110> log likelihood: -3837\n",
      "INFO:lda:<120> log likelihood: -3828\n",
      "INFO:lda:<130> log likelihood: -3835\n",
      "INFO:lda:<140> log likelihood: -3825\n",
      "INFO:lda:<150> log likelihood: -3830\n",
      "INFO:lda:<160> log likelihood: -3847\n",
      "INFO:lda:<170> log likelihood: -3836\n",
      "INFO:lda:<180> log likelihood: -3811\n",
      "INFO:lda:<190> log likelihood: -3823\n",
      "INFO:lda:<200> log likelihood: -3836\n",
      "INFO:lda:<210> log likelihood: -3833\n",
      "INFO:lda:<220> log likelihood: -3812\n",
      "INFO:lda:<230> log likelihood: -3817\n",
      "INFO:lda:<240> log likelihood: -3827\n",
      "INFO:lda:<250> log likelihood: -3811\n",
      "INFO:lda:<260> log likelihood: -3828\n",
      "INFO:lda:<270> log likelihood: -3825\n",
      "INFO:lda:<280> log likelihood: -3827\n",
      "INFO:lda:<290> log likelihood: -3820\n",
      "INFO:lda:<300> log likelihood: -3848\n",
      "INFO:lda:<310> log likelihood: -3804\n",
      "INFO:lda:<320> log likelihood: -3813\n",
      "INFO:lda:<330> log likelihood: -3822\n",
      "INFO:lda:<340> log likelihood: -3877\n",
      "INFO:lda:<350> log likelihood: -3828\n",
      "INFO:lda:<360> log likelihood: -3829\n",
      "INFO:lda:<370> log likelihood: -3836\n",
      "INFO:lda:<380> log likelihood: -3861\n",
      "INFO:lda:<390> log likelihood: -3827\n",
      "INFO:lda:<400> log likelihood: -3829\n",
      "INFO:lda:<410> log likelihood: -3810\n",
      "INFO:lda:<420> log likelihood: -3839\n",
      "INFO:lda:<430> log likelihood: -3826\n",
      "INFO:lda:<440> log likelihood: -3866\n",
      "INFO:lda:<450> log likelihood: -3867\n",
      "INFO:lda:<460> log likelihood: -3827\n",
      "INFO:lda:<470> log likelihood: -3837\n",
      "INFO:lda:<480> log likelihood: -3828\n",
      "INFO:lda:<490> log likelihood: -3837\n",
      "INFO:lda:<499> log likelihood: -3861\n",
      "INFO:lda:n_documents: 100\n",
      "INFO:lda:vocab_size: 129\n",
      "INFO:lda:n_words: 657\n",
      "INFO:lda:n_topics: 5\n",
      "INFO:lda:n_iter: 500\n",
      "WARNING:lda:all zero row in document-term matrix found\n",
      "D:\\miniconda\\envs\\tf\\lib\\site-packages\\lda\\utils.py:55: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if sparse and not np.issubdtype(doc_word.dtype, int):\n",
      "INFO:lda:<0> log likelihood: -5215\n",
      "INFO:lda:<10> log likelihood: -3432\n",
      "INFO:lda:<20> log likelihood: -3361\n",
      "INFO:lda:<30> log likelihood: -3349\n",
      "INFO:lda:<40> log likelihood: -3324\n",
      "INFO:lda:<50> log likelihood: -3307\n",
      "INFO:lda:<60> log likelihood: -3271\n",
      "INFO:lda:<70> log likelihood: -3241\n",
      "INFO:lda:<80> log likelihood: -3248\n",
      "INFO:lda:<90> log likelihood: -3268\n",
      "INFO:lda:<100> log likelihood: -3276\n",
      "INFO:lda:<110> log likelihood: -3273\n",
      "INFO:lda:<120> log likelihood: -3269\n",
      "INFO:lda:<130> log likelihood: -3251\n",
      "INFO:lda:<140> log likelihood: -3283\n",
      "INFO:lda:<150> log likelihood: -3282\n",
      "INFO:lda:<160> log likelihood: -3274\n",
      "INFO:lda:<170> log likelihood: -3262\n",
      "INFO:lda:<180> log likelihood: -3259\n",
      "INFO:lda:<190> log likelihood: -3270\n",
      "INFO:lda:<200> log likelihood: -3283\n",
      "INFO:lda:<210> log likelihood: -3250\n",
      "INFO:lda:<220> log likelihood: -3244\n",
      "INFO:lda:<230> log likelihood: -3251\n",
      "INFO:lda:<240> log likelihood: -3252\n",
      "INFO:lda:<250> log likelihood: -3277\n",
      "INFO:lda:<260> log likelihood: -3311\n",
      "INFO:lda:<270> log likelihood: -3303\n",
      "INFO:lda:<280> log likelihood: -3265\n",
      "INFO:lda:<290> log likelihood: -3286\n",
      "INFO:lda:<300> log likelihood: -3234\n",
      "INFO:lda:<310> log likelihood: -3226\n",
      "INFO:lda:<320> log likelihood: -3240\n",
      "INFO:lda:<330> log likelihood: -3274\n",
      "INFO:lda:<340> log likelihood: -3255\n",
      "INFO:lda:<350> log likelihood: -3206\n",
      "INFO:lda:<360> log likelihood: -3248\n",
      "INFO:lda:<370> log likelihood: -3285\n",
      "INFO:lda:<380> log likelihood: -3248\n",
      "INFO:lda:<390> log likelihood: -3276\n",
      "INFO:lda:<400> log likelihood: -3267\n",
      "INFO:lda:<410> log likelihood: -3284\n",
      "INFO:lda:<420> log likelihood: -3315\n",
      "INFO:lda:<430> log likelihood: -3263\n",
      "INFO:lda:<440> log likelihood: -3265\n",
      "INFO:lda:<450> log likelihood: -3237\n",
      "INFO:lda:<460> log likelihood: -3212\n",
      "INFO:lda:<470> log likelihood: -3263\n",
      "INFO:lda:<480> log likelihood: -3229\n",
      "INFO:lda:<490> log likelihood: -3206\n",
      "INFO:lda:<499> log likelihood: -3231\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lda.lda.LDA at 0x217a4edcfc8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pre-decided by the requirement of LDA algorithm\n",
    "topic_num=5\n",
    "\n",
    "#tokenization\n",
    "pros_tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "cons_tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')                               \n",
    "#read the dataset                \n",
    "#docs=open('news.txt').readlines()\n",
    "\n",
    "#transform the docs into a count matrix\n",
    "#get teh vocabulary\n",
    "pros_matrix = pros_tf_vectorizer.fit_transform(pros)\n",
    "pros_vocab = pros_tf_vectorizer.get_feature_names()\n",
    "\n",
    "cons_matrix = cons_tf_vectorizer.fit_transform(cons)\n",
    "cons_vocab = cons_tf_vectorizer.get_feature_names()\n",
    "\n",
    "#get the vocabulary\n",
    "#vocab=tf_vectorizer.get_feature_names()\n",
    "\n",
    "#initialize the LDA model\n",
    "pro_model = lda.LDA(n_topics=topic_num, n_iter=500)\n",
    "con_model = lda.LDA(n_topics=topic_num, n_iter=500)\n",
    "\n",
    "#fit the model to the dataset\n",
    "pro_model.fit(pros_matrix)\n",
    "con_model.fit(cons_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[148  59  58]\n",
      "-------------------- 153\n",
      "148\n",
      "59\n",
      "58\n",
      "[104  76  98]\n",
      "-------------------- 153\n",
      "104\n",
      "76\n",
      "98\n",
      "[ 29  22 150]\n",
      "-------------------- 153\n",
      "29\n",
      "22\n",
      "150\n",
      "[  5 105 113]\n",
      "-------------------- 153\n",
      "5\n",
      "105\n",
      "113\n",
      "[ 58  81 104]\n",
      "-------------------- 153\n",
      "58\n",
      "81\n",
      "104\n"
     ]
    }
   ],
   "source": [
    "topic_keywords = {}\n",
    "#write the top terms for each topic\n",
    "#ancillary\n",
    "top_words_num=3\n",
    "pro_topic_mixes= pro_model.topic_word_\n",
    "fw=open('pro_top_terms_per_topic.txt','w')\n",
    "for i in range(topic_num):#for each topic\n",
    "    top_indexes=np.argsort(pro_topic_mixes[i])[::-1][:top_words_num]                              \n",
    "    my_top=''\n",
    "    for ind in top_indexes:\n",
    "        my_top+=pros_vocab[ind]+' '\n",
    "        if i in topic_keywords:\n",
    "            topic_keywords[i].append(pros_vocab[ind])\n",
    "        else:\n",
    "            topic_keywords[i] = [pros_vocab[ind]]\n",
    "    fw.write('TOPIC: '+str(i)+' --> '+str(my_top)+'\\n')\n",
    "fw.close()\n",
    "\n",
    "doc_topic = {}\n",
    "doc_top_topic = {}\n",
    "#write the top topics for each doc\n",
    "top_topics_num=3\n",
    "pro_doc_mixes= pro_model.doc_topic_\n",
    "fw=open('pro_topic_mixture_per_doc.txt','w')\n",
    "for i in range(len(pro_doc_mixes)):#for each doc\n",
    "    top_indexes=np.argsort(pro_doc_mixes[i])[::-1][:top_topics_num]     \n",
    "    my_top=''\n",
    "    for ind in top_indexes:\n",
    "        temp_topic = ind\n",
    "        temp_likelihood = round(pro_doc_mixes[i][ind], 2)\n",
    "        likelihood_top = -1\n",
    "        topic_top = -1\n",
    "        if temp_likelihood > likelihood_top:\n",
    "            likelihood_top = temp_likelihood\n",
    "            topic_top = temp_topic\n",
    "        my_top+=' '+str(ind)+':'+str(round(pro_doc_mixes[i][ind],2))\n",
    "        if i in doc_topic:\n",
    "            doc_topic[i].append((ind, round(pro_doc_mixes[i][ind], 2)))\n",
    "        else:\n",
    "            doc_topic[i] = [(ind, round(pro_doc_mixes[i][ind], 2))]\n",
    "    doc_top_topic[i] = topic_top\n",
    "    fw.write('DOC: '+str(i)+' --> '+str(my_top)+'\\n')\n",
    "fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: ['work', 'great', 'good'], 1: ['people', 'job', 'opportunities'], 2: ['day', 'company', 'working'], 3: ['amazon', 'place', 'really'], 4: ['good', 'learn', 'people']}\n",
      "{0: 0, 1: 3, 2: 3, 3: 3, 4: 3, 5: 1, 6: 0, 7: 3, 8: 1, 9: 2, 10: 0, 11: 4, 12: 3, 13: 3, 14: 4, 15: 2, 16: 2, 17: 3, 18: 4, 19: 3, 20: 4, 21: 0, 22: 4, 23: 4, 24: 3, 25: 4, 26: 3, 27: 3, 28: 2, 29: 3, 30: 2, 31: 2, 32: 4, 33: 3, 34: 3, 35: 4, 36: 3, 37: 3, 38: 2, 39: 3, 40: 3, 41: 3, 42: 3, 43: 3, 44: 3, 45: 3, 46: 2, 47: 3, 48: 4, 49: 0, 50: 2, 51: 3, 52: 3, 53: 3, 54: 3, 55: 4, 56: 2, 57: 3, 58: 3, 59: 3, 60: 3, 61: 4, 62: 1, 63: 2, 64: 3, 65: 3, 66: 3, 67: 3, 68: 4, 69: 2, 70: 3, 71: 4, 72: 4, 73: 4, 74: 3, 75: 0, 76: 2, 77: 4, 78: 1, 79: 2, 80: 3, 81: 0, 82: 3, 83: 2, 84: 2, 85: 3, 86: 3, 87: 4, 88: 3, 89: 4, 90: 3, 91: 3, 92: 4, 93: 4, 94: 3, 95: 3, 96: 3, 97: 4, 98: 1, 99: 3}\n",
      "{0: 7, 3: 50, 1: 5, 2: 16, 4: 22}\n",
      "{7: 0, 50: 3, 5: 1, 16: 2, 22: 4}\n",
      "TOP  1 comments' keywords are  ['people', 'job', 'opportunities']\n",
      "TOP  2 comments' keywords are  ['amazon', 'place', 'really']\n"
     ]
    }
   ],
   "source": [
    "print(topic_keywords)\n",
    "print(doc_top_topic)\n",
    "\n",
    "k_user = 2 #user parameter\n",
    "invers_count = {}\n",
    "for key, value in doc_top_topic.items():\n",
    "    if value in invers_count:\n",
    "        invers_count[value] += 1\n",
    "    else:\n",
    "        invers_count[value] = 1\n",
    "print(invers_count)\n",
    "count_dic = {}\n",
    "for k,v in invers_count.items():\n",
    "    count_dic[v] = k\n",
    "print(count_dic)\n",
    "\n",
    "for i in range(k_user):    \n",
    "    l = len(count_dic.keys())\n",
    "    i_topicNum = count_dic[sorted(count_dic.keys())[l-i-1]]                \n",
    "    print(\"TOP \", i+1, \" pro comments' keywords are \", topic_keywords[i_topicNum])\n",
    "    \n",
    "    for docj, portion in doc_topic.items():\n",
    "        for j, posb in portion:\n",
    "            if j == i_topicNum and posb > 0.8:\n",
    "                print(\"e.g. \", pros[docj])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: ['fast', 'environment', 'hard'], 1: ['email', 'continue', 'reading'], 2: ['work', 'time', 'long'], 3: ['company', 'bad', 'management'], 4: ['email', 'resend', 'continue']}\n",
      "{0: 4, 1: 3, 2: 2, 3: 4, 4: 2, 5: 2, 6: 4, 7: 3, 8: 0, 9: 2, 10: 4, 11: 2, 12: 3, 13: 2, 14: 0, 15: 3, 16: 3, 17: 2, 18: 2, 19: 2, 20: 2, 21: 3, 22: 1, 23: 4, 24: 4, 25: 4, 26: 4, 27: 3, 28: 2, 29: 4, 30: 4, 31: 3, 32: 3, 33: 3, 34: 2, 35: 3, 36: 3, 37: 4, 38: 3, 39: 2, 40: 2, 41: 4, 42: 4, 43: 3, 44: 4, 45: 3, 46: 3, 47: 3, 48: 2, 49: 0, 50: 3, 51: 3, 52: 3, 53: 3, 54: 4, 55: 2, 56: 3, 57: 2, 58: 2, 59: 3, 60: 2, 61: 2, 62: 2, 63: 2, 64: 3, 65: 4, 66: 2, 67: 2, 68: 3, 69: 3, 70: 3, 71: 3, 72: 2, 73: 4, 74: 3, 75: 1, 76: 3, 77: 2, 78: 0, 79: 2, 80: 3, 81: 4, 82: 3, 83: 4, 84: 1, 85: 2, 86: 4, 87: 2, 88: 3, 89: 2, 90: 4, 91: 3, 92: 4, 93: 4, 94: 2, 95: 3, 96: 2, 97: 2, 98: 4, 99: 2}\n",
      "{4: 24, 3: 35, 2: 34, 0: 4, 1: 3}\n",
      "{24: 4, 35: 3, 34: 2, 4: 0, 3: 1}\n",
      "TOP  1  con comments' keywords are  ['company', 'bad', 'management']\n",
      "e.g.  big company so things can get lost\n",
      "e.g.  Ensuring we utilize Frugality as one of our Leadership Principles more effectively, what we lack in.\n",
      "e.g.  Large company with some evidence of bureaucracy building.\n",
      "e.g.  It can get repetitive but itâ€™s really not that bad\n",
      "e.g.  Frugality as a leadership principle can be good and bad compared to benefits at other FAANG companies. Stock vesting schedule.\n",
      "e.g.  Lots of extra processes involved for product to reach the customers\n",
      "e.g.  large company that doesn't care all that much\n",
      "e.g.  Company is too big to move\n",
      "TOP  2  con comments' keywords are  ['work', 'time', 'long']\n",
      "e.g.  Long hours very physical work\n",
      "e.g.  no cons they are a great place to work\n",
      "e.g.  Work life balance is tricky here\n",
      "e.g.  long hours and physically demanding work\n",
      "e.g.  On your feet for 12+ hours a day, many reports and Washes to complete daily.\n",
      "e.g.  You will have no social life or time for yourself.\n",
      "e.g.  ** not great for long time career path\n",
      "e.g.  I liked working at Souq that years.\n",
      "e.g.  always in the need of operating at the next level\n",
      "e.g.  It takes time to become a full-time employee\n",
      "e.g.  Long hours standing in one place.\n"
     ]
    }
   ],
   "source": [
    "topic_keywords = {}\n",
    "#write the top terms for each topic\n",
    "#ancillary\n",
    "top_words_num=3\n",
    "con_topic_mixes= con_model.topic_word_\n",
    "fw=open('con_top_terms_per_topic.txt','w')\n",
    "for i in range(topic_num):#for each topic\n",
    "    top_indexes=np.argsort(con_topic_mixes[i])[::-1][:top_words_num]                              \n",
    "    my_top=''\n",
    "    for ind in top_indexes:\n",
    "        my_top+=cons_vocab[ind]+' '\n",
    "        if i in topic_keywords:\n",
    "            topic_keywords[i].append(cons_vocab[ind])\n",
    "        else:\n",
    "            topic_keywords[i] = [cons_vocab[ind]]\n",
    "    fw.write('TOPIC: '+str(i)+' --> '+str(my_top)+'\\n')\n",
    "fw.close()\n",
    "\n",
    "doc_topic = {}\n",
    "doc_top_topic = {}\n",
    "#write the top topics for each doc\n",
    "top_topics_num=3\n",
    "con_doc_mixes= con_model.doc_topic_\n",
    "fw=open('con_topic_mixture_per_doc.txt','w')\n",
    "for i in range(len(con_doc_mixes)):#for each doc\n",
    "    top_indexes=np.argsort(con_doc_mixes[i])[::-1][:top_topics_num]     \n",
    "    my_top=''\n",
    "    for ind in top_indexes:\n",
    "        temp_topic = ind\n",
    "        temp_likelihood = round(con_doc_mixes[i][ind], 2)\n",
    "        likelihood_top = -1\n",
    "        topic_top = -1\n",
    "        if temp_likelihood > likelihood_top:\n",
    "            likelihood_top = temp_likelihood\n",
    "            topic_top = temp_topic\n",
    "        my_top+=' '+str(ind)+':'+str(round(con_doc_mixes[i][ind],2))\n",
    "        if i in doc_topic:\n",
    "            doc_topic[i].append((ind, round(con_doc_mixes[i][ind], 2)))\n",
    "        else:\n",
    "            doc_topic[i] = [(ind, round(con_doc_mixes[i][ind], 2))]\n",
    "    doc_top_topic[i] = topic_top\n",
    "    fw.write('DOC: '+str(i)+' --> '+str(my_top)+'\\n')\n",
    "fw.close()\n",
    "\n",
    "print(topic_keywords)\n",
    "print(doc_top_topic)\n",
    "\n",
    "k_user = 2 #user parameter\n",
    "invers_count = {}\n",
    "for key, value in doc_top_topic.items():\n",
    "    if value in invers_count:\n",
    "        invers_count[value] += 1\n",
    "    else:\n",
    "        invers_count[value] = 1\n",
    "print(invers_count)\n",
    "count_dic = {}\n",
    "for k,v in invers_count.items():\n",
    "    count_dic[v] = k\n",
    "print(count_dic)\n",
    "\n",
    "for i in range(k_user):\n",
    "    l = len(count_dic.keys())\n",
    "    i_topicNum = count_dic[sorted(count_dic.keys())[l-i-1]]                \n",
    "    print(\"TOP \", i+1, \" con comments' keywords are \", topic_keywords[i_topicNum])\n",
    "    \n",
    "    for docj, portion in doc_topic.items():\n",
    "        for j, posb in portion:\n",
    "            if j == i_topicNum and posb > 0.8:\n",
    "                print(\"e.g. \", cons[docj])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
